{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB2bdCYyTIcv",
        "outputId": "af95ea33-f7ae-48ed-c436-c9ba8595b690"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BeztYEKRTM-d"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data to test code\n",
        "# !tar xvzf /content/drive/MyDrive/GP_project/part3.tar.gz"
      ],
      "metadata": {
        "id": "UZBg40kDTaSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##export pytorch model to onnx \n",
        "# ! pip install onnxruntime\n",
        "# !pip --quiet install onnx onnxruntime onnxsim\n",
        "# !pip install onnx-tf\n",
        "# !git clone https://github.com/deepcam-cn/yolov5-face.git\n",
        "# %cd /content/yolov5-face\n",
        "# !git clone https://github.com/hpc203/yolov5-face-landmarks-opencv-v2.git\n",
        "# !python /content/yolov5-face/main_export_onnx.py --cfg /content/yolov5-face/models/yolov5s.yaml --weights /content/drive/MyDrive/GP_project/Weights/yolov5s-face.pt --image /content/yolov5-face-landmarks-opencv-v2/selfie.jpg  \n"
      ],
      "metadata": {
        "id": "Nc_sz_DxThyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install --upgrade opencv-python\n",
        "# print(cv2. __version__ )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niCBth8Mse73",
        "outputId": "1c57f777-9677-49e2-9e10-8cf1b75d8955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## to copy model file from drive to notebook\n",
        "# %cp /content/drive/MyDrive/GP_project/Weights/yolov5s-face.onnx /content/yolov5s-face.onnx"
      ],
      "metadata": {
        "id": "-MmU4xEXaJ0G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run onnx model using opencv2**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S6AS_itt8uN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _make_grid(nx=20, ny=20):\n",
        "        xv, yv = np.meshgrid(np.arange(ny), np.arange(nx))\n",
        "        return np.stack((xv, yv), 2).reshape((-1, 2)).astype(np.float32)\n",
        "        \n",
        "\n",
        "\n",
        "def drawPred(frame, conf, left, top, right, bottom, landmark):\n",
        "      \"\"\"plot predicted bounding box and facial landmarks on the image\"\"\"\n",
        "      # Draw a bounding box.\n",
        "      cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), thickness=2)\n",
        "      label = '%.2f' % conf\n",
        "      #Display the label at the top of the bounding box\n",
        "      cv2.putText(frame,label,(left, top - 2),cv2.FONT_HERSHEY_SIMPLEX,0.75,[225, 255, 255],thickness=2) \n",
        "      for i in range(5):\n",
        "          cv2.circle(frame, (landmark[i*2], landmark[i*2+1]), 1, (0,255,0), thickness=-1)\n",
        "      return frame \n",
        "\n",
        "\n",
        "def align_faces(im,bbox, landmark=None):\n",
        "    \"\"\"align predicted face using 5 facial landmarks \n",
        "       and crop it with shape (112,112).\n",
        "       if landmarks don't exis, we crop the image using bounding box \"\"\"\n",
        "    M = None\n",
        "    warped=None\n",
        "    x1 = int(bbox[0])  # rect.left()\n",
        "    y1 = int(bbox[1])  # rect.top()\n",
        "    x2 = int(bbox[2])  # rect.right()\n",
        "    y2 = int(bbox[3])   # rect.bottom()\n",
        "    \n",
        "    if landmark is not None:\n",
        "        src = np.array([\n",
        "          [30.2946, 51.6963],\n",
        "          [65.5318, 51.5014],\n",
        "          [48.0252, 71.7366],\n",
        "          [33.5493, 92.3655],\n",
        "          [62.7299, 92.2041] ], dtype=np.float32 )\n",
        "        src[:,0] += 8.0\n",
        "        dst = landmark.astype(np.float32)\n",
        "        M = cv2.estimateAffine2D(dst,src)[0]\n",
        "        warped = cv2.warpAffine(im,M,(112,112), borderValue = 0.0)\n",
        "    else:\n",
        "        warped = im[y1:y1+y2, x1:x1+x2]\n",
        "    return warped\n"
      ],
      "metadata": {
        "id": "PudsejZJ9LBU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def non_max_supression(frame, outs ,confThreshold=0.3,nmsThreshold=0.45):\n",
        "        frameHeight = frame.shape[0]\n",
        "        frameWidth = frame.shape[1]\n",
        "        \n",
        "        ratioh, ratiow = frameHeight / 640, frameWidth / 640\n",
        "        # Scan through all the bounding boxes output from the network and keep only the\n",
        "        # ones with high confidence scores. Assign the box's class label as the class with the highest score.\n",
        "\n",
        "        confidences = []\n",
        "        boxes = []\n",
        "        landmarks = []\n",
        "        #print(outs.shape)\n",
        "        for detection in outs:\n",
        "      \n",
        "            #confidence = detection[15]\n",
        "            conf=detection[4]*detection[15]\n",
        "            #print(conf)\n",
        "            \n",
        "            if detection[4] > confThreshold:\n",
        "                center_x = int(detection[0] * ratiow)\n",
        "                center_y = int(detection[1] * ratioh)\n",
        "                width = int(detection[2] * ratiow)\n",
        "                height = int(detection[3] * ratioh)\n",
        "                left = int(center_x - width / 2)\n",
        "                top = int(center_y - height / 2)\n",
        "\n",
        "                confidences.append(float(conf))\n",
        "                boxes.append([left, top, width, height])\n",
        "                landmark = detection[5:15] * np.tile(np.float32([ratiow,ratioh]), 5)\n",
        "                landmarks.append(landmark.astype(np.int32))\n",
        "        # Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
        "        # lower confidences.\n",
        "        indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
        "        \n",
        "        return np.array(boxes)[indices],np.array(landmarks)[indices],np.array(confidences)[indices]"
      ],
      "metadata": {
        "id": "9ereLepnslqy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(frame,boxes,landmarks,confidences):\n",
        "        \"\"\"filter predicted bboxes, \n",
        "             if exist more than face in the image \n",
        "            ,take the closeset face to the camera using IOD metric\n",
        "            and return image cropped and aligned.\n",
        "            else if there is no faces predicted return None \"\"\"\n",
        "        im=frame.copy()\n",
        "        #im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "        max_iod=-1\n",
        "        box_=None\n",
        "        land_mark=None\n",
        "        for box, landmark, confidence in zip(boxes, landmarks, confidences):\n",
        "            lm = [tuple(landmark[i:i + 2]) for i in range(0, len(landmark), 2)]\n",
        "            IOD = abs((lm[0][0] + lm[0][1] / 2.0) - (lm[1][0] + lm[1][1] / 2.0)) #This metric could be used to infer distance from the camera.\n",
        "            if IOD > max_iod:  # filter bbox based on IOD(inter-ocular distance)\n",
        "                  max_iod=IOD\n",
        "                  box_=box\n",
        "                  land_mark=lm \n",
        "            ## draw bounding box and landmarks       \n",
        "            #frame = drawPred(frame, confidence, box[0], box[1], box[0] +  box[2], box[1] + box[3], landmark)\n",
        "        if max_iod != -1:\n",
        "            cropped=align_faces(im.copy(),box_, np.array(land_mark))\n",
        "            # plt.imshow(cropped)\n",
        "            # plt.show()\n",
        "            # path='/content/cropped.jpg'\n",
        "            # cv2.imwrite(path,cropped)    \n",
        "            return cropped\n",
        "        else:\n",
        "           return None   "
      ],
      "metadata": {
        "id": "g-ZtmqZWArrA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_anchors(prediction,inpWidth,inpHeight):\n",
        "        anchors = [[4,5,  8,10,  13,16], [23,29,  43,55,  73,105], [146,217,  231,300,  335,433]]\n",
        "        num_classes = 1\n",
        "        nl = len(anchors)\n",
        "        na = len(anchors[0]) // 2\n",
        "        no = num_classes + 5 + 10\n",
        "        grid = [np.zeros(1)] * nl\n",
        "        stride = np.array([8., 16., 32.])\n",
        "        anchor_grid = np.asarray(anchors, dtype=np.float32).reshape(nl, -1, 2)\n",
        "        \n",
        "        prediction[..., [0,1,2,3,4,15]] = 1 / (1 + np.exp(-prediction[..., [0,1,2,3,4,15]]))   ###sigmoid\n",
        "        row_ind = 0\n",
        "        for i in range(nl):\n",
        "            h, w = int(inpHeight/stride[i]), int(inpWidth/stride[i])\n",
        "            length = int(na * h * w)\n",
        "            if grid[i].shape[2:4] != (h,w):\n",
        "                grid[i] = _make_grid(w, h)\n",
        "            \n",
        "            g_i = np.tile(grid[i], (na, 1))\n",
        "            a_g_i = np.repeat(anchor_grid[i], h * w, axis=0)\n",
        "            prediction[row_ind:row_ind + length, 0:2] = (prediction[row_ind:row_ind + length, 0:2] * 2. - 0.5 + g_i) * int(stride[i])\n",
        "            prediction[row_ind:row_ind + length, 2:4] = (prediction[row_ind:row_ind + length, 2:4] * 2) ** 2 * a_g_i\n",
        "\n",
        "            prediction[row_ind:row_ind + length, 5:7] = prediction[row_ind:row_ind + length, 5:7] * a_g_i + g_i * int(stride[i])   # landmark x1 y1\n",
        "            prediction[row_ind:row_ind + length, 7:9] = prediction[row_ind:row_ind + length, 7:9] * a_g_i + g_i * int(stride[i])  # landmark x2 y2\n",
        "            prediction[row_ind:row_ind + length, 9:11] = prediction[row_ind:row_ind + length, 9:11] * a_g_i + g_i * int(stride[i])  # landmark x3 y3\n",
        "            prediction[row_ind:row_ind + length, 11:13] = prediction[row_ind:row_ind + length, 11:13] * a_g_i + g_i * int(stride[i])  # landmark x4 y4\n",
        "            prediction[row_ind:row_ind + length, 13:15] = prediction[row_ind:row_ind + length, 13:15] * a_g_i + g_i * int(stride[i])  # landmark x5 y5\n",
        "            row_ind += length\n",
        "        return prediction   \n"
      ],
      "metadata": {
        "id": "w5RrYN8N99h6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect(srcimg,model_path,inpWidth,inpHeight):\n",
        "        \n",
        "        #prepare input image\n",
        "        blob = cv2.dnn.blobFromImage(srcimg, 1 / 255.0, (inpWidth, inpHeight), [0, 0, 0], swapRB=True, crop=False)\n",
        "        # read model\n",
        "        net = cv2.dnn.readNet(model_path)\n",
        "        # Sets the input to the network\n",
        "        net.setInput(blob)\n",
        "\n",
        "        # Runs the forward pass to get output of the output layers\n",
        "        outs = net.forward(net.getUnconnectedOutLayersNames())[0]\n",
        "        #print(outs[0, [0,1,2,3,4,15]])\n",
        "        # inference output\n",
        "        outs=process_anchors(outs,inpWidth,inpHeight)\n",
        "        \n",
        "        return outs"
      ],
      "metadata": {
        "id": "HJuWd3gP83ks"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yolov5(srcimg,model_path='/content/yolov5s-face.onnx',confThreshold=0.3,nmsThreshold=0.45):\n",
        "  #srcimg = cv2.imread(imgpath)\n",
        "  # imname=imgpath.split('/')[-1]\n",
        "  # save_path+='/'+imname\n",
        "  dets = detect(srcimg,model_path,640,640)\n",
        "  boxes,landmarks,confidences=non_max_supression(srcimg, dets)\n",
        "  img = postprocess(srcimg,boxes,landmarks,confidences)\n",
        "  return img\n",
        "  # if img is not None:\n",
        "  #   cv2.imwrite(save_path, img)\n",
        "  # else:\n",
        "  #   print(\"there is no faces in the image\")    "
      ],
      "metadata": {
        "id": "6GG9WETNBRas"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #test onnx model\n",
        "# from tqdm import tqdm\n",
        "# import os\n",
        "# for img in tqdm(os.listdir('/content/part3')):\n",
        "#     path = os.path.join('/content/part3', img)\n",
        "#     img_data = cv2.imread(path) # BGR IMAGE\n",
        "#     img_data = cv2.cvtColor(img_data, cv2.COLOR_BGR2RGB)\n",
        "#     img=yolov5(img_data)"
      ],
      "metadata": {
        "id": "jOCC8UxWP4fp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**convert to tflite model**"
      ],
      "metadata": {
        "id": "7a5U64tQPKgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export onnx model to tflite \n",
        "# !onnx-tf convert -i /content/yolov5s-face.onnx -o /content/\n",
        "# converter = tf.lite.TFLiteConverter.from_saved_model('/content/')\n",
        "# tflite_model = converter.convert()\n",
        "\n",
        "# with open('/content/yolov5s_model.tflite', 'wb') as f:\n",
        "#   f.write(tflite_model)"
      ],
      "metadata": {
        "id": "2UEga5m9ARJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_tf_model(srcimg,model_path,inpWidth,inpHeight):\n",
        "   \n",
        "   #prepare input     \n",
        "   blob = cv2.dnn.blobFromImage(srcimg, 1 / 255.0, (inpWidth, inpHeight), [0, 0, 0], swapRB=True, crop=False)\n",
        "   \n",
        "\n",
        "   interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "   #Allocate tensors.\n",
        "   interpreter.allocate_tensors()\n",
        "   # Get input and output tensors.\n",
        "   input_details = interpreter.get_input_details()\n",
        "   output_details = interpreter.get_output_details()\n",
        "\n",
        "   # Test the model on random input data.\n",
        "   input_shape = input_details[0]['shape']\n",
        "   interpreter.set_tensor(input_details[0]['index'], blob)\n",
        "\n",
        "   interpreter.invoke()\n",
        "\n",
        "   # The function `get_tensor()` returns a copy of the tensor data.\n",
        "   # Use `tensor()` in order to get a pointer to the tensor.\n",
        "   output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "   output_data=process_anchors(output_data,inpWidth,inpHeight)\n",
        "  \n",
        "\n",
        "   return output_data        \n"
      ],
      "metadata": {
        "id": "S7HZISuZJzca"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##test tflite model\n",
        "# model_path='/content/yolov5s_model.tflite'\n",
        "# imgpath='/content/3faces.PNG'\n",
        "# srcimg = cv2.imread(imgpath)\n",
        "# dets = detect_tf_model(srcimg,model_path,640,640)\n",
        "# boxes,landmarks,confidences=non_max_supression(srcimg, dets)\n",
        "# img = postprocess(srcimg,boxes,landmarks,confidences)\n",
        "# if img is not None:\n",
        "#     cv2.imwrite('/content/img.jpg', img)\n",
        "# else:\n",
        "#   print(\"there is no faces in the image\") "
      ],
      "metadata": {
        "id": "ZxpqZYa6PkXA"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}